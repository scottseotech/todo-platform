{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Reference Platform","text":""},{"location":"#overview","title":"Overview","text":"<p>The Reference Platform is a demonstration of production-grade infrastructure patterns and practices. This is not a tutorial\u2014it's a fully-functional reference implementation that showcases how modern cloud-native systems are built and operated.</p> <p>What this demonstrates: - GitOps-driven deployments with ArgoCD - Self-hosted CI/CD with GitHub Actions Runner Controller - Full-stack observability (traces, logs, metrics) - ChatOps for operational workflows - Infrastructure as Code with Kubernetes operators - AI integration via Model Context Protocol</p> <p>What this is NOT: - A step-by-step tutorial - A production-ready product to clone - A comprehensive guide to DevOps practices</p> <p>This platform runs a simple todo application as a reference workload\u2014the value is in the infrastructure, automation, and operational patterns, not the application itself.</p>"},{"location":"#platform-architecture","title":"Platform Architecture","text":"<pre><code>graph TB\n    subgraph \"Interface Layer\"\n        AI[AI Assistant&lt;br/&gt;Natural Language]\n    end\n\n    subgraph \"Application Layer\"\n        MCP[MCP Server&lt;br/&gt;AI Integration]\n        API[REST API&lt;br/&gt;Go + Gin]\n    end\n\n    subgraph \"Data Layer\"\n        DB[(PostgreSQL&lt;br/&gt;High Availability)]\n    end\n\n    subgraph \"Infrastructure\"\n        GitOps[ArgoCD&lt;br/&gt;GitOps]\n        CI[GitHub Actions&lt;br/&gt;Self-Hosted]\n        Obs[Observability&lt;br/&gt;Full Stack]\n    end\n\n    AI --&gt; MCP --&gt; API --&gt; DB\n    GitOps -.manages.-&gt; API\n    CI -.builds.-&gt; API\n    Obs -.monitors.-&gt; API\n    Obs -.monitors.-&gt; MCP\n    Obs -.monitors.-&gt; DB</code></pre>"},{"location":"#key-demonstrations","title":"Key Demonstrations","text":"<p>Each component showcases a different production capability:</p> <p>AI Integration via Model Context Protocol (MCP) The platform uses the emerging MCP standard instead of proprietary AI APIs. Demonstrates: abstraction layers, protocol design, and vendor independence for AI integrations.</p> <p>GitOps with ArgoCD Deployments happen through Git commits, not manual kubectl commands. Demonstrates: infrastructure as code, declarative configuration, audit trails, and continuous reconciliation.</p> <p>Self-Hosted CI/CD Runners GitHub Actions runners execute inside the Kubernetes cluster using Actions Runner Controller. Demonstrates: cost optimization, security isolation, in-cluster integration.</p> <p>Full-Stack Observability OpenTelemetry distributed tracing with two trace flows: (1) Slack Bot \u2192 MCP Server, (2) MCP Server \u2192 API \u2192 Database. Complete trace-to-log correlation across all services. Demonstrates: distributed tracing, W3C trace propagation, trace-to-log correlation, metrics collection, and correlation across signals.</p> <p>Automated Database Operations CloudNativePG handles PostgreSQL high availability, backups, and failover automatically. Demonstrates: Kubernetes operator patterns, stateful workloads, and database reliability.</p>"},{"location":"#technology-stack","title":"Technology Stack","text":"Layer Technology Why It Matters Interface Slack + OpenAI Natural language interaction, modern AI integration Application Go, Gin, MCP Performance, type safety, emerging AI protocols Data PostgreSQL, CloudNativePG Production-grade database with automated HA Deployment ArgoCD, Kustomize GitOps workflow, declarative infrastructure CI/CD GitHub Actions (ARC) Self-hosted runners, cost control, in-cluster access Observability OpenTelemetry, Tempo, Loki, Prometheus End-to-end tracing, full telemetry stack"},{"location":"#reference-workload-todo-application","title":"Reference Workload: Todo Application","text":"<p>The platform runs a simple todo list application to anchor the infrastructure demonstrations:</p> <ul> <li>Backend: Go REST API with PostgreSQL (CloudNativePG)</li> <li>AI Integration: MCP server for natural language operations via Slack</li> <li>Interface: Slack ChatOps for operational control and user interaction</li> </ul> <p>The application is intentionally minimal\u2014complex enough to demonstrate database operations, API patterns, distributed tracing, and AI integration, but simple enough to keep focus on the platform capabilities rather than application logic.</p>"},{"location":"#explore-the-platform","title":"Explore the Platform","text":"<p>Architecture Overview \u2192 Detailed system design with component descriptions and data flows.</p> <p>Deployment Documentation \u2192 See how GitOps, self-hosted runners, and ArgoCD work together for automated deployments.</p> <p>Observability Stack \u2192 Explore distributed tracing with two trace flows and complete trace-to-log correlation across all services.</p> <p>Development Setup \u2192 Run the platform locally with hot reloading and development tools.</p>"},{"location":"#why-this-platform-exists","title":"Why This Platform Exists","text":"<p>Building a todo app is simple. Building a reference platform that demonstrates:</p> <ul> <li>AI integration without vendor lock-in</li> <li>Automated deployments with zero downtime</li> <li>Debugging distributed systems in production</li> <li>Managing stateful workloads reliably</li> </ul> <p>...that's what separates platform engineers from application developers.</p> <p>This platform demonstrates those skills through working infrastructure, not theoretical explanations.</p>"},{"location":"api/todo-api/","title":"Todo API","text":"<p>The Todo API provides RESTful endpoints for managing todos with full observability and cloud-native patterns.</p> <p>View Full API Documentation \u2192</p>"},{"location":"api/todo-mcp/","title":"Todo MCP API","text":"<p>The MCP server provides Model Context Protocol endpoints for AI-powered todo management integration.</p> <p>View Full API Documentation \u2192</p>"},{"location":"architecture/adr/","title":"Architecture Decision Records","text":""},{"location":"architecture/adr/#why-document-decisions","title":"Why Document Decisions","text":"<p>Production systems evolve over time. Architecture Decision Records (ADRs) capture the reasoning behind key technical choices, making it easier for teams to understand:</p> <ul> <li>What decisions were made</li> <li>Why those decisions were made</li> <li>What alternatives were considered</li> <li>What tradeoffs were accepted</li> </ul> <p>This prevents repeated debates, helps onboard new team members, and provides historical context for future refactoring.</p>"},{"location":"architecture/adr/#adr-000-use-a-todo-list-application-as-the-core-demo","title":"ADR-000: Use a Todo List Application as the Core Demo","text":"<p>Status: Accepted</p> Show details <p>Context:</p> <p>The goal of this project is not to build another task management application, but to demonstrate end-to-end production practices across modern DevOps disciplines \u2014 including infrastructure as code, GitOps, CI/CD, observability, and self-hosted automation runners.</p> <p>A lightweight app such as a todo list provides just enough complexity (API, database, UI) to anchor the supporting infrastructure while staying simple enough to keep focus on the platform engineering aspects.</p> <p>Decision:</p> <p>Use a simple todo list application as the reference workload for demonstrating:</p> <ul> <li>Kubernetes-based deployment</li> <li>ArgoCD-driven GitOps workflows</li> <li>ChatOps integrations via Slack for operational feedback and control</li> <li>Observability stack (Prometheus, Grafana, Tempo, Loki)</li> <li>CI/CD pipelines executing on a self-hosted GitHub Actions Runner Controller (ARC)</li> <li>Documentation and automation with MkDocs, Kustomzie, and IaC tooling</li> </ul> <p>The application itself remains minimal \u2014 serving as a stable target to validate automation, reliability, and environment consistency.</p> <p>Alternatives Considered:</p> <ol> <li>Custom complex microservice project \u2013 Would showcase more application logic but obscure the infrastructure focus and slow iteration.</li> <li>Off-the-shelf benchmark app (e.g., Sock Shop, Hipster Shop) \u2013 Provides realistic architecture but adds unnecessary moving parts and less control over the full lifecycle.</li> </ol> <p>Consequences:</p> <p>\u2705 Positive:</p> <ul> <li>Keeps cognitive load low while demonstrating full production-grade DevOps stack</li> <li>Enables reproducible demos of CI/CD, GitOps, and ChatOps workflows</li> <li>Clear isolation of infrastructure competency from application logic</li> <li>Simplifies troubleshooting, monitoring, and performance testing</li> </ul> <p>\u274c Negative:</p> <ul> <li>Application is not innovative or feature-rich, so less compelling as a product demo</li> <li>May undersell application-level engineering depth to non-technical audiences</li> <li>Requires extra explanation that the value lies in platform automation, not app features</li> </ul>"},{"location":"architecture/adr/#adr-001-gitops-with-argocd","title":"ADR-001: GitOps with ArgoCD","text":"<p>Status: Accepted</p> Show details <p>Context:</p> <p>Deployments need to be:</p> <ul> <li>Reproducible and auditable</li> <li>Automated without manual kubectl commands</li> <li>Version-controlled with rollback capability</li> <li>Accessible to the entire team without requiring cluster credentials</li> </ul> <p>Decision:</p> <p>Use ArgoCD for GitOps-based continuous deployment where:</p> <ul> <li>Git repository is the single source of truth</li> <li>All infrastructure and application manifests are declared in Git</li> <li>ArgoCD automatically syncs cluster state to match Git</li> <li>Deployments happen through Git commits, not imperative commands</li> </ul> <p>Alternatives Considered:</p> <ol> <li>Manual kubectl - Not scalable, no audit trail, requires cluster access</li> <li>Helm + CI/CD - Imperative, doesn't continuously reconcile state</li> </ol> <p>Consequences:</p> <p>\u2705 Positive:</p> <ul> <li>Complete audit trail of all changes</li> <li>Easy rollback via Git revert</li> <li>No need to distribute kubeconfig</li> <li>Self-healing (ArgoCD reconciles drift)</li> <li>Visual UI for deployment status</li> </ul> <p>\u274c Negative:</p> <ul> <li>Additional component to manage</li> <li>Learning curve for GitOps workflow</li> <li>Slightly slower than direct kubectl apply</li> </ul>"},{"location":"architecture/adr/#adr-002-self-hosted-github-actions-runners","title":"ADR-002: Self-Hosted GitHub Actions Runners","text":"<p>Status: Accepted</p> Show details <p>Context:</p> <p>CI/CD pipelines need to:</p> <ul> <li>Build and push Docker images</li> <li>Update Kubernetes manifests</li> <li>Verify deployments in the cluster</li> <li>Control costs for high-frequency builds</li> </ul> <p>GitHub-hosted runners have limitations:</p> <ul> <li>No access to in-cluster resources</li> <li>Limited to public IP addresses</li> <li>No persistent caching</li> <li>Can be expensive at scale for private repos</li> </ul> <p>Decision:</p> <p>Use Actions Runner Controller (ARC) to run self-hosted GitHub Actions runners inside the Kubernetes cluster.</p> <p>Alternatives Considered:</p> <ol> <li>GitHub-hosted runners - No cluster access, higher cost, no caching</li> <li>Jenkins - More complex, requires separate infrastructure</li> <li>GitLab CI - Would require migrating from GitHub</li> </ol> <p>Consequences:</p> <p>\u2705 Positive:</p> <ul> <li>Direct kubectl access for deployment verification</li> <li>Faster builds with Docker layer caching</li> <li>Lower cost (runs on existing cluster resources)</li> <li>Custom runner images with pre-installed tools</li> <li>Auto-scaling based on job queue</li> </ul> <p>\u274c Negative:</p> <ul> <li>Additional infrastructure to maintain</li> <li>Need to manage runner lifecycle</li> </ul>"},{"location":"architecture/adr/#adr-003-opentelemetry-for-observability","title":"ADR-003: OpenTelemetry for Observability","text":"<p>Status: Accepted</p> Show details <p>Context:</p> <p>Distributed systems need comprehensive observability:</p> <ul> <li>Trace requests across service boundaries</li> <li>Correlate logs with traces</li> <li>Monitor performance metrics</li> <li>Avoid vendor lock-in with proprietary agents</li> </ul> <p>Decision:</p> <p>Use OpenTelemetry SDK for instrumentation with:</p> <ul> <li>OTLP protocol for traces \u2192 Tempo</li> <li>Structured JSON logs \u2192 Fluent Bit \u2192 Loki</li> <li>Prometheus metrics exposition</li> <li>W3C Trace Context propagation</li> </ul> <p>Alternatives Considered:</p> <ol> <li>New Relic APM - Proprietary, expensive, vendor lock-in</li> </ol> <p>Consequences:</p> <p>\u2705 Positive:</p> <ul> <li>Vendor-neutral (can swap backends without code changes)</li> <li>Industry standard (CNCF graduated project)</li> <li>Single SDK for traces, metrics, and logs</li> <li>W3C standard trace propagation</li> <li>Strong community and tooling support</li> </ul> <p>\u274c Negative:</p> <ul> <li>Slightly more complex than vendor SDKs</li> <li>Performance overhead (mitigated with sampling)</li> <li>Need to run backend infrastructure (Tempo, Loki, Prometheus)</li> </ul>"},{"location":"architecture/adr/#adr-004-cloudnativepg-for-postgresql","title":"ADR-004: CloudNativePG for PostgreSQL","text":"<p>Status: Accepted</p> Show details <p>Context:</p> <p>Application needs a production-grade PostgreSQL database with:</p> <ul> <li>High availability and automatic failover</li> <li>Automated backups and point-in-time recovery</li> <li>Declarative Kubernetes-native configuration</li> <li>Minimal operational overhead</li> </ul> <p>Decision:</p> <p>Use CloudNativePG operator to manage PostgreSQL clusters:</p> <ul> <li>Declarative cluster definition</li> <li>Automatic failover and self-healing</li> <li>Continuous backup to S3 (MinIO)</li> <li>Built-in monitoring and metrics</li> </ul> <p>Alternatives Considered:</p> <ol> <li>StatefulSet + manual setup - No automation, complex failover</li> </ol> <p>Consequences:</p> <p>\u2705 Positive:</p> <ul> <li>Fully automated HA with no manual intervention</li> <li>Backup automation with retention policies</li> <li>Kubernetes-native (CRDs and operators)</li> <li>Active CNCF sandbox project</li> <li>Great documentation and community</li> </ul> <p>\u274c Negative:</p> <ul> <li>Operator adds complexity</li> <li>Learning curve for CNPG concepts</li> </ul>"},{"location":"architecture/adr/#adr-005-model-context-protocol-mcp-for-ai-integration","title":"ADR-005: Model Context Protocol (MCP) for AI Integration","text":"<p>Status: Accepted</p> Show details <p>Context:</p> <p>Platform needs AI-powered natural language interface:</p> <ul> <li>Allow users to manage todos via conversation</li> <li>Integrate with modern LLMs (OpenAI, Claude, etc.)</li> <li>Avoid tight coupling to specific AI vendors</li> <li>Provide structured tool calling interface</li> </ul> <p>Decision:</p> <p>Implement Model Context Protocol (MCP) server:</p> <ul> <li>Standardized protocol for AI-application integration</li> <li>Tool-based interface for todo operations</li> <li>Compatible with any MCP-compliant AI system</li> <li>Clean abstraction between AI and business logic</li> </ul> <p>Alternatives Considered:</p> <ol> <li>LangChain - Heavy framework</li> </ol> <p>Consequences:</p> <p>\u2705 Positive:</p> <ul> <li>Future-proof against AI vendor changes</li> <li>Industry standard protocol</li> <li>Clean separation of concerns</li> <li>Reusable across different AI frontends</li> <li>Demonstrates modern AI integration patterns</li> </ul> <p>\u274c Negative: - Emerging standard (not fully mature) - Additional service to maintain</p>"},{"location":"architecture/adr/#adr-006-kustomize-over-helm","title":"ADR-006: Kustomize over Helm","text":"<p>Status: Accepted</p> Show details <p>Context:</p> <p>Kubernetes manifests need:</p> <ul> <li>Environment-specific customization (dev, staging, prod)</li> <li>Image version management</li> <li>No complex templating logic</li> <li>Integration with GitOps workflow</li> </ul> <p>Decision:</p> <p>Use Kustomize for manifest management:</p> <ul> <li>Declarative patches over base manifests</li> <li>Built-in image tag management</li> <li>Native kubectl integration</li> <li>ArgoCD has first-class Kustomize support</li> </ul> <p>Alternatives Considered:</p> <ol> <li>Helm - Template complexity, values.yaml indirection</li> <li>Plain YAML - No reusability, duplication across environments</li> </ol> <p>Consequences:</p> <p>\u2705 Positive: - Simple overlay model (easy to understand) - No templating language to learn - Built into kubectl - Clean image version updates via CLI - ArgoCD native support</p> <p>\u274c Negative: - Less powerful than Helm for complex scenarios - No package registry/versioning - Cannot conditionally include resources easily</p>"},{"location":"architecture/adr/#adr-007-minio-for-s3-compatible-storage","title":"ADR-007: MinIO for S3-Compatible Storage","text":"<p>Status: Accepted</p> Show details <p>Context:</p> <p>Platform needs object storage for:</p> <ul> <li>PostgreSQL continuous backups (CNPG)</li> <li>Log archival (Loki)</li> <li>Potential future use cases (file uploads, exports)</li> <li>Local development and testing</li> <li>Cost control</li> </ul> <p>Decision:</p> <p>Use MinIO as S3-compatible object storage:</p> <ul> <li>Self-hosted within Kubernetes cluster</li> <li>S3 API compatibility</li> <li>Integrated with CNPG and Loki</li> </ul> <p>Alternatives Considered:</p> <ol> <li>AWS S3 - Vendor lock-in, requires AWS account, egress costs</li> <li>Local filesystem - Not durable, no replication</li> </ol> <p>Consequences:</p> <p>\u2705 Positive:</p> <ul> <li>S3 API compatibility (portable to cloud if needed)</li> <li>Self-contained (no external dependencies)</li> <li>No egress costs</li> <li>Great for local development</li> </ul> <p>\u274c Negative:</p> <ul> <li>Additional service to manage</li> <li>Need to handle backup/disaster recovery for MinIO itself</li> </ul>"},{"location":"architecture/adr/#future-adrs","title":"Future ADRs","text":"<p>As the platform evolves, consider documenting:</p> <ul> <li>ADR-008: Monitoring and alerting rules</li> </ul>"},{"location":"architecture/adr/#adr-template","title":"ADR Template","text":"<p>When adding new ADRs, use this structure:</p> <pre><code>## ADR-XXX: [Short Title]\n\n**Status:** [Proposed | Accepted | Deprecated | Superseded]\n\n??? note \"Show details\"\n    **Context:**\n\n    What is the issue that we're seeing that is motivating this decision or change?\n\n    **Decision:**\n\n    What is the change that we're proposing and/or doing?\n\n    **Alternatives Considered:**\n\n    1. **Option 1** - Why not chosen\n    2. **Option 2** - Why not chosen\n\n    **Consequences:**\n\n    \u2705 **Positive:**\n\n    - What becomes easier\n    - What improves\n\n    \u274c **Negative:**\n\n    - What becomes more difficult\n    - What tradeoffs we're accepting\n</code></pre>"},{"location":"architecture/adr/#related-documentation","title":"Related Documentation","text":"<ul> <li>Architecture Overview</li> <li>Deployment Guide</li> <li>Observability</li> </ul>"},{"location":"architecture/c4/","title":"C4 Architecture Diagrams","text":"<p>Best Viewed in Light Mode</p> <p>For optimal visibility of diagram labels and relationship text, switch to light mode using the theme toggle in the header (\u2600\ufe0f/\ud83c\udf19 icon).</p>"},{"location":"architecture/c4/#about-c4-model","title":"About C4 Model","text":"<p>The C4 model provides a hierarchical approach to documenting software architecture through four levels of abstraction: Context, Containers, Components, and Code. These diagrams help visualize the system at different zoom levels\u2014from high-level system landscape to detailed component interactions.</p> <p>This platform uses C4 diagrams to demonstrate:</p> <ul> <li>How external systems interact with the platform (Context)</li> <li>What containers (deployable units) make up the system (Containers)</li> <li>How components within services are organized (Components)</li> </ul>"},{"location":"architecture/c4/#level-1-system-context","title":"Level 1: System Context","text":"<p>The highest level view showing how the Reference Platform fits into the broader ecosystem.</p> <pre><code>C4Context\n    title System Context - Reference Platform\n\n    Person(user, \"Platform User\", \"Developer or operator managing applications\")\n    Person(enduser, \"End User\", \"User interacting via Slack\")\n\n    System(platform, \"Reference Platform\", \"Kubernetes-based platform demonstrating GitOps, observability, and ChatOps patterns\")\n\n    System_Ext(github, \"GitHub\", \"Source control and CI/CD automation\")\n    System_Ext(dockerhub, \"Docker Hub\", \"Container image registry\")\n    System_Ext(slack, \"Slack\", \"Chat interface for operations\")\n    System_Ext(openai, \"OpenAI\", \"LLM provider for AI features\")\n\n    Rel(user, platform, \"Deploys code, monitors\", \"Git, kubectl, Grafana\")\n    Rel(enduser, slack, \"Manages todos\", \"Slack messages\")\n    Rel(slack, platform, \"Sends commands\", \"MCP protocol\")\n    Rel(platform, openai, \"Processes natural language\", \"OpenAI API\")\n    Rel(platform, github, \"Triggers workflows, fetches code\", \"GitHub API\")\n    Rel(platform, dockerhub, \"Pulls images\", \"Docker Registry API\")\n    Rel(github, dockerhub, \"Pushes built images\", \"Docker Registry API\")\n\n    UpdateLayoutConfig($c4ShapeInRow=\"3\", $c4BoundaryInRow=\"1\")</code></pre> <p>Key Interactions:</p> <ul> <li>Users deploy via Git \u2192 GitHub Actions \u2192 ArgoCD \u2192 Kubernetes</li> <li>End users interact via Slack \u2192 MCP Server \u2192 REST API</li> <li>Platform pulls images from Docker Hub for deployments</li> <li>LLM processes natural language commands through OpenAI</li> </ul>"},{"location":"architecture/c4/#level-2-container-diagram","title":"Level 2: Container Diagram","text":"<p>Shows the high-level technical building blocks (containers) that make up the platform.</p> <pre><code>C4Container\n    title Container Diagram - Reference Platform\n\n    Person(user, \"End User\", \"Manages todos via Slack\")\n    Person(operator, \"Operator\", \"Manages platform\")\n\n    System_Boundary(platform, \"Reference Platform\") {\n        Container(slackbot, \"Slack Bot\", \"Python, Bolt SDK\", \"Handles Slack events and commands\")\n        Container(mcp, \"MCP Server\", \"Go\", \"Model Context Protocol server for AI tool calling\")\n        Container(api, \"Todo API\", \"Go, Gin\", \"REST API for todo operations\")\n        ContainerDb(db, \"Database\", \"PostgreSQL, CNPG\", \"Stores todo data with HA\")\n\n        Container(argocd, \"ArgoCD\", \"Kubernetes Operator\", \"GitOps continuous deployment\")\n        Container(arc, \"ARC Runners\", \"Kubernetes, GitHub Actions\", \"Self-hosted CI/CD runners\")\n\n        Container(tempo, \"Tempo\", \"Grafana Tempo\", \"Distributed tracing storage\")\n        Container(loki, \"Loki\", \"Grafana Loki\", \"Log aggregation and storage\")\n        Container(prometheus, \"Prometheus\", \"Prometheus\", \"Metrics collection\")\n        Container(grafana, \"Grafana\", \"Grafana\", \"Unified observability dashboard\")\n\n        Container(minio, \"MinIO\", \"S3-compatible storage\", \"Backup storage for DB and logs\")\n    }\n\n    System_Ext(github, \"GitHub\", \"Source control and workflows\")\n    System_Ext(dockerhub, \"Docker Hub\", \"Container registry\")\n    System_Ext(openai, \"OpenAI API\", \"LLM provider\")\n\n    Rel(user, slackbot, \"Sends commands\", \"Slack API\")\n    Rel(slackbot, openai, \"Processes NL\", \"HTTPS\")\n    Rel(slackbot, mcp, \"Invokes tools\", \"HTTP/MCP\")\n    Rel(mcp, api, \"CRUD operations\", \"HTTP/REST\")\n    Rel(api, db, \"Reads/writes\", \"PostgreSQL\")\n\n    Rel(operator, grafana, \"Views metrics/traces\", \"HTTPS\")\n    Rel(api, tempo, \"Sends traces\", \"OTLP/gRPC\")\n    Rel(api, prometheus, \"Exposes metrics\", \"/metrics\")\n    Rel(mcp, tempo, \"Sends traces\", \"OTLP/gRPC\")\n    Rel(mcp, prometheus, \"Exposes metrics\", \"/metrics\")\n\n    Rel(grafana, tempo, \"Queries traces\", \"HTTP\")\n    Rel(grafana, loki, \"Queries logs\", \"HTTP\")\n    Rel(grafana, prometheus, \"Queries metrics\", \"PromQL\")\n\n    Rel(db, minio, \"Continuous backup\", \"S3 API\")\n    Rel(loki, minio, \"Stores logs\", \"S3 API\")\n\n    Rel(github, arc, \"Dispatches jobs\", \"GitHub API\")\n    Rel(arc, dockerhub, \"Pushes images\", \"Docker API\")\n    Rel(argocd, github, \"Syncs manifests\", \"Git\")\n\n    UpdateLayoutConfig($c4ShapeInRow=\"4\", $c4BoundaryInRow=\"2\")</code></pre> <p>Key Containers:</p> <ul> <li>Application Services: Slack Bot, MCP Server, Todo API</li> <li>Data Storage: PostgreSQL (CNPG), MinIO</li> <li>Platform Services: ArgoCD, ARC Runners</li> <li>Observability: Tempo, Loki, Prometheus, Grafana</li> </ul>"},{"location":"architecture/c4/#level-3-component-diagram-todo-api","title":"Level 3: Component Diagram - Todo API","text":"<p>Detailed view of components within the Todo API service.</p> <pre><code>C4Component\n    title Component Diagram - Todo API Service\n\n    Person(client, \"API Client\", \"MCP Server or direct HTTP client\")\n\n    Container_Boundary(api, \"Todo API\") {\n        Component(router, \"HTTP Router\", \"Gin\", \"Routes HTTP requests to handlers\")\n        Component(handler, \"Todo Handlers\", \"Go\", \"HTTP request handlers for CRUD operations\")\n        Component(service, \"Todo Service\", \"Go\", \"Business logic for todo operations\")\n        Component(repository, \"Todo Repository\", \"Go, GORM\", \"Data access layer\")\n        Component(otel, \"OpenTelemetry\", \"OTEL SDK\", \"Trace instrumentation\")\n        Component(metrics, \"Metrics Exporter\", \"Prometheus\", \"Exposes /metrics endpoint\")\n    }\n\n    ContainerDb(db, \"PostgreSQL\", \"CloudNativePG\", \"Todo data storage\")\n    Container(tempo, \"Tempo\", \"Trace collector\", \"Receives traces\")\n    Container(prometheus, \"Prometheus\", \"Metrics scraper\", \"Scrapes /metrics\")\n\n    Rel(client, router, \"HTTP requests\", \"REST/JSON\")\n    Rel(router, handler, \"Dispatch\", \"Function call\")\n    Rel(handler, service, \"Business operations\", \"Function call\")\n    Rel(service, repository, \"Data operations\", \"Function call\")\n    Rel(repository, db, \"SQL queries\", \"PostgreSQL protocol\")\n\n    Rel(handler, otel, \"Create spans\", \"OTEL SDK\")\n    Rel(service, otel, \"Create spans\", \"OTEL SDK\")\n    Rel(otel, tempo, \"Export traces\", \"OTLP/gRPC :4317\")\n\n    Rel(metrics, prometheus, \"Scraped by\", \"HTTP :8080/metrics\")\n\n    UpdateLayoutConfig($c4ShapeInRow=\"3\", $c4BoundaryInRow=\"1\")</code></pre> <p>Component Responsibilities:</p> <ul> <li>Router: Request routing with middleware (CORS, logging, recovery)</li> <li>Handlers: HTTP-specific logic, request validation, response serialization</li> <li>Service: Business rules, transaction management, error handling</li> <li>Repository: Database queries, ORM mapping, connection pooling</li> <li>OpenTelemetry: Span creation, context propagation, trace export</li> <li>Metrics: Prometheus metrics (request counts, latencies, errors)</li> </ul>"},{"location":"architecture/c4/#level-3-component-diagram-mcp-server","title":"Level 3: Component Diagram - MCP Server","text":"<p>Detailed view of components within the MCP Server.</p> <pre><code>C4Component\n    title Component Diagram - MCP Server\n\n    Person(ai, \"AI Assistant\", \"OpenAI or compatible LLM\")\n\n    Container_Boundary(mcp, \"MCP Server\") {\n        Component(server, \"MCP Server\", \"Go, MCP SDK\", \"Handles MCP protocol requests\")\n        Component(tools, \"Tool Registry\", \"Go\", \"Registers and dispatches tool calls\")\n        Component(todotool, \"Todo Tools\", \"Go\", \"MCP tools for todo operations\")\n        Component(client, \"API Client\", \"Go, HTTP client\", \"Communicates with Todo API\")\n        Component(otel, \"OpenTelemetry\", \"OTEL SDK\", \"Trace instrumentation\")\n    }\n\n    Container(api, \"Todo API\", \"REST API\", \"Backend service\")\n    Container(tempo, \"Tempo\", \"Trace collector\", \"Receives traces\")\n\n    Rel(ai, server, \"Tool invocations\", \"MCP/JSON-RPC\")\n    Rel(server, tools, \"Dispatch tool\", \"Function call\")\n    Rel(tools, todotool, \"Execute\", \"Function call\")\n    Rel(todotool, client, \"HTTP requests\", \"REST/JSON\")\n    Rel(client, api, \"API calls\", \"HTTP\")\n\n    Rel(server, otel, \"Create spans\", \"OTEL SDK\")\n    Rel(todotool, otel, \"Create spans\", \"OTEL SDK\")\n    Rel(otel, tempo, \"Export traces\", \"OTLP/gRPC :4317\")\n\n    UpdateLayoutConfig($c4ShapeInRow=\"3\", $c4BoundaryInRow=\"1\")</code></pre> <p>Component Responsibilities:</p> <ul> <li>MCP Server: Protocol handling, JSON-RPC transport, tool discovery</li> <li>Tool Registry: Tool registration, validation, dispatch</li> <li>Todo Tools: MCP tool implementations (add, list, complete, delete todos)</li> <li>API Client: HTTP client with retry logic, timeout handling</li> <li>OpenTelemetry: Trace context extraction/injection, span lifecycle</li> </ul>"},{"location":"architecture/c4/#deployment-diagram","title":"Deployment Diagram","text":"<p>Shows how containers are deployed to infrastructure.</p> <pre><code>C4Deployment\n    title Deployment Diagram - Kubernetes Cluster\n\n    Deployment_Node(k8s, \"Kubernetes Cluster\", \"K3s\") {\n        Deployment_Node(default_ns, \"Namespace: default\") {\n            Container(slackbot, \"Slack Bot\", \"Deployment\")\n            Container(mcp, \"MCP Server\", \"Deployment\")\n            Container(api, \"Todo API\", \"Deployment\")\n        }\n\n        Deployment_Node(cnpg_ns, \"Namespace: cnpg\") {\n            ContainerDb(db, \"PostgreSQL Cluster\", \"StatefulSet, 2 replicas\")\n        }\n\n        Deployment_Node(argocd_ns, \"Namespace: argocd\") {\n            Container(argocd, \"ArgoCD\", \"Deployment\")\n        }\n\n        Deployment_Node(arc_ns, \"Namespace: arc-runners\") {\n            Container(arc, \"GitHub Runners\", \"ScaleSet, 2-5 replicas\")\n        }\n\n        Deployment_Node(monitoring_ns, \"Namespace: monitoring\") {\n            Container(grafana, \"Grafana\", \"Deployment\")\n            Container(prometheus, \"Prometheus\", \"StatefulSet\")\n            Container(tempo, \"Tempo\", \"Deployment\")\n            Container(loki, \"Loki\", \"StatefulSet\")\n        }\n\n        Deployment_Node(minio_ns, \"Namespace: minio\") {\n            Container(minio, \"MinIO\", \"StatefulSet, 4 replicas\")\n        }\n    }\n\n    Deployment_Node(external, \"External Services\") {\n        System_Ext(github, \"GitHub\")\n        System_Ext(dockerhub, \"Docker Hub\")\n        System_Ext(slack, \"Slack\")\n    }\n\n    Rel(argocd, github, \"Git sync\")\n    Rel(arc, github, \"Job execution\")\n    Rel(arc, dockerhub, \"Image push\")\n    Rel(slackbot, slack, \"Events\")\n\n    UpdateLayoutConfig($c4ShapeInRow=\"2\", $c4BoundaryInRow=\"1\")</code></pre> <p>Deployment Characteristics:</p> <ul> <li>High Availability: Database with 2 replicas, auto-failover</li> <li>Auto-Scaling: ARC runners scale 2-5 based on job queue</li> <li>Resource Isolation: Separate namespaces for different concerns</li> <li>StatefulSets: Used for databases and storage requiring persistent volumes</li> <li>Deployments: Used for stateless application services</li> </ul>"},{"location":"architecture/c4/#benefits-of-c4-diagrams","title":"Benefits of C4 Diagrams","text":"<p>For this platform:</p> <ul> <li>Context: Shows how external systems integrate (GitHub, Slack, OpenAI)</li> <li>Containers: Reveals the microservices architecture and infrastructure services</li> <li>Components: Demonstrates internal service organization and patterns</li> <li>Deployment: Illustrates Kubernetes topology and scaling characteristics</li> </ul> <p>For recruiters/engineers:</p> <ul> <li>Hierarchical views from business context to technical detail</li> <li>Clear separation of concerns across namespaces</li> <li>Observable patterns: tracing, metrics, logging flows</li> <li>Production patterns: HA databases, auto-scaling, GitOps</li> </ul>"},{"location":"architecture/c4/#related-documentation","title":"Related Documentation","text":"<ul> <li>Architecture Overview</li> <li>Architecture Decision Records</li> <li>Deployment Documentation</li> <li>Observability Stack</li> </ul>"},{"location":"architecture/hr/","title":"Architecture","text":"<pre><code>graph LR\n  subgraph Interface\n    Slack[Slack App]\n    UX[AI Features]\n  end\n\n  subgraph Services\n    API[App Services]\n    Orchestrator[Automation &amp; Deploys]\n  end\n\n  subgraph Platform\n    Data[(Database)]\n    Storage[(Object Storage)]\n    Telemetry[Telemetry &amp; Alerts]\n    Dashboards[Dashboards]\n  end\n\n  Slack --&gt; UX\n  UX --&gt; API\n  Orchestrator --&gt; API\n  API --&gt; Data\n  API --&gt; Storage\n  API --&gt; Telemetry\n  Telemetry --&gt; Dashboards</code></pre>"},{"location":"architecture/overview/","title":"Architecture Overview","text":"<p>The Todo Platform is a cloud-native Kubernetes application demonstrating production-ready DevOps practices including GitOps, comprehensive observability, and automated database operations.</p> <p>Tip</p> <p>Click the node description to jump to the detailed explanation section.</p>"},{"location":"architecture/overview/#system-architecture","title":"System Architecture","text":"<pre><code>graph TB\n    subgraph \"Interface Layer\"\n        SU[Slack App]\n        SS[Slack Server]\n        AI[AI Assistant]\n        AM[LLM]\n    end\n\n    subgraph \"Application Layer\"\n        MCP[MCP Server]\n        API[REST API]\n    end\n\n    subgraph \"Data Layer\"\n        PG[(PostgreSQL&lt;br/&gt;Database)]\n        MIN[(MinIO/S3)]\n    end\n\n    subgraph \"Observability Layer\"\n        Tempo[Tempo&lt;br/&gt;Traces]\n        Loki[Loki&lt;br/&gt;Logs]\n        Prom[Prometheus&lt;br/&gt;Metrics]\n        Graf[Grafana&lt;br/&gt;Dashboards]\n    end\n\n    SU --&gt; SS\n\n    SS --&gt; AI\n    AI --&gt; MCP\n    MCP --&gt; API\n    API --&gt; PG\n\n    API -.traces.-&gt; Tempo\n    MCP -.traces.-&gt; Tempo\n    API -.logs.-&gt; Loki\n    MCP -.logs.-&gt; Loki\n    API -.metrics.-&gt; Prom\n\n    Tempo --&gt; Graf\n    Loki --&gt; Graf\n    Prom --&gt; Graf\n\n    click MCP \"/architecture/overview/#todo-mcp-server\" \"tool tip\"\n    click API \"/architecture/overview/#todo-rest-api\" \"tool tip\"\n\n    click SU \"/architecture/overview/#interface-layer\" \"tool tip\"\n    click SS \"/architecture/overview/#interface-layer\" \"tool tip\"\n    click AI \"/architecture/overview/#interface-layer\" \"tool tip\"\n    click AM \"/architecture/overview/#interface-layer\" \"tool tip\"\n\n    click PG \"/architecture/overview/#postgresql-database\" \"tool tip\"\n    click MIN \"/architecture/overview/#minio-s3\" \"tool tip\"\n\n    click Tempo \"/architecture/overview/#tempo-traces\" \"tool tip\"\n    click Loki \"/architecture/overview/#loki-logs\" \"tool tip\"\n    click Prom \"/architecture/overview/#prometheus-metrics\" \"tool tip\"\n    click Graf \"/architecture/overview/#grafana-dashboard\" \"tool tip\"</code></pre>"},{"location":"architecture/overview/#interface-layer","title":"Interface Layer","text":"<p>The interface layer provides user-facing access to the todo platform through AI-powered natural language interaction via Slack.</p> Show more details <p>Components:</p> <ul> <li>Slack App - User-facing Slack application where users send natural language commands and receive responses</li> <li>Slack Server - Slack's infrastructure that receives events and routes messages between users and the AI assistant</li> <li>AI Assistant - Intelligent agent that interprets user intent, invokes MCP tools, and formats responses in natural language</li> <li>LLM - Large language model that powers the AI assistant's natural language understanding and generation capabilities</li> </ul> <p>User Flow:</p> <ol> <li>User sends a message in Slack (e.g., \"Add buy groceries to my todo list\")</li> <li>Slack Server forwards the event to the AI Assistant</li> <li>AI Assistant uses the LLM to interpret the user's intent</li> <li>LLM determines which MCP tool to invoke (e.g., <code>todos-add</code>)</li> <li>AI Assistant calls the MCP Server with structured parameters</li> <li>Response flows back through the AI Assistant, which formats a natural language reply</li> <li>User receives the result in Slack</li> </ol> <p>This architecture enables users to manage todos through conversational AI without needing to learn commands or syntax.</p>"},{"location":"architecture/overview/#application-layer","title":"Application Layer","text":""},{"location":"architecture/overview/#todo-mcp-server","title":"Todo MCP Server","text":"Show more details <p>MCP server providing AI-friendly tool interfaces for todo operations. Implements the full MCP specification including:</p> <p>Tools - Four core operations:</p> <ul> <li><code>todos-add</code> - Create new todos</li> <li><code>todos-list</code> - Query existing todos</li> <li><code>todos-update</code> - Modify todo status/details</li> <li><code>todos-delete</code> - Remove todos</li> </ul> <p>Resources - Dynamic todo data exposure:</p> <ul> <li><code>todos://with-due-date</code> - Real-time todo list with due date as structured resource</li> </ul> <p>Prompts - Pre-configured interaction templates for AI assistants:</p> <ul> <li><code>todos-add</code> - Guided prompt for creating new todos with proper context</li> <li><code>todos-update</code> - Guided prompt for updating existing todos</li> </ul> <p>Prompts demonstrate full MCP specification compliance and can be tested directly in VS Code with GitHub Copilot's MCP integration.</p> <p>Built with Go, supports both HTTP and SSE transports, and instrumented with OpenTelemetry for distributed tracing.</p>"},{"location":"architecture/overview/#todo-rest-api","title":"Todo Rest API","text":"Show more details <p>RESTful API service built with Go and the Gin framework. Provides CRUD endpoints for todo management with:</p> <ul> <li>GORM ORM for database operations</li> <li>OpenTelemetry instrumentation for traces</li> <li>Automatic Prometheus metrics exposure</li> <li>Database connection pooling</li> </ul>"},{"location":"architecture/overview/#data-layer","title":"Data Layer","text":""},{"location":"architecture/overview/#backup-flow","title":"Backup Flow","text":"<ul> <li>CNPG operator performs daily PostgreSQL backups</li> <li>Point-in-time recovery available from backup archives</li> </ul>"},{"location":"architecture/overview/#postgresql-database","title":"PostgreSQL Database","text":"Show more details <p>High-availability PostgreSQL cluster managed by the CloudNativePG operator:</p> <ul> <li>2-instance cluster for redundancy</li> <li>Automated daily backups to MinIO S3 (2 AM UTC)</li> <li>Point-in-time recovery capability</li> <li>Read-write service: <code>todo-db-rw.cnpg.svc.cluster.local</code></li> </ul>"},{"location":"architecture/overview/#minio-s3","title":"MinIO S3","text":"Show more details <p>S3-compatible object storage serving dual purposes:</p> <ul> <li>PostgreSQL Backups: Daily CNPG automated backups with retention policies</li> <li>Log Storage: Loki long-term log retention</li> </ul>"},{"location":"architecture/overview/#observability-layer","title":"Observability Layer","text":""},{"location":"architecture/overview/#observability-flow","title":"Observability Flow","text":"<ul> <li>Traces: Each request generates spans across MCP \u2192 API \u2192 Database, collected by Tempo</li> <li>Metrics: Prometheus scrapes all services, storing time-series data</li> <li>Logs: Fluent Bit ships logs to Loki, which stores in MinIO</li> <li>Alerts: Grafana evaluates alert rules and notifies Slack on threshold violations</li> </ul>"},{"location":"architecture/overview/#tempo-traces","title":"Tempo - Traces","text":"Show more details <p>Collects distributed traces via OpenTelemetry:</p> <ul> <li>Two distributed trace flows: (1) Slack Bot \u2192 MCP Server via HTTP/SSE, (2) MCP Server \u2192 API \u2192 Database via HTTP/REST</li> <li>MCP protocol creates a natural trace boundary between flows</li> <li>W3C Trace Context propagation within each flow</li> <li>OTLP gRPC ingestion (port 4317)</li> <li>Complete trace-to-log correlation: logs include trace_id and span_id for all services (todo-bot, todo-mcp, todo-api)</li> </ul>"},{"location":"architecture/overview/#loki-logs","title":"Loki - Logs","text":"Show more details <p>Centralized log aggregation with S3 storage:</p> <ul> <li>Collects structured logs from all services</li> <li>Fluent Bit agents for log shipping</li> <li>MinIO S3 backend for long-term retention</li> <li>Full-text search and filtering in Grafana</li> </ul>"},{"location":"architecture/overview/#prometheus-metrics","title":"Prometheus - Metrics","text":"Show more details <p>Scrapes metrics from all services every 15 seconds:</p> <ul> <li>Todo MCP Server: General golang runtime metrics</li> <li>Todo API: General golang runtime metrics</li> <li>PostgreSQL: Database performance, replication lag, operational stats</li> </ul>"},{"location":"architecture/overview/#grafana-dashboard","title":"Grafana - Dashboard","text":"Show more details <p>Unified observability dashboard:</p> <ul> <li>Queries Prometheus for metrics visualization</li> <li>Queries Tempo for trace exploration</li> <li>Queries Loki for log analysis</li> <li>Alert routing to Slack for critical events</li> <li>Correlation between traces, metrics, and logs</li> </ul>"},{"location":"chatops/chatops/","title":"ChatOps","text":"<p>ChatOps extends DevOps by bringing automation and collaboration into the same space where teams communicate \u2014 usually Slack. Instead of context-switching between terminals and dashboards, developers and operators interact with infrastructure through conversations. The goal is transparency, shared ownership, and faster feedback loops.</p>"},{"location":"chatops/chatops/#what-chatops-means-in-this-project","title":"What ChatOps Means in This Project","text":"<p>This demo integrates ChatOps patterns to demonstrate how operational workflows can be initiated, observed, and audited directly within Slack.</p> Area ChatOps Role Release Management Developers can trigger releases via an interactive Slack modal that dispatches GitHub workflows. Monitoring &amp; Incident Response Scheduled jobs and observability alerts post summarized errors and status updates to Slack channels. Automation Transparency Every triggered workflow or alert is posted to Slack for visibility \u2014 creating a shared timeline of operational activity. Self-Service Infrastructure Routine actions (deployments, rollbacks, tag cuts) can be performed without special permissions or terminal access."},{"location":"chatops/chatops/#why-it-matters","title":"Why It Matters","text":"<ul> <li>Reduces Friction \u2014 common operational actions happen in Slack, no need to jump between tools.</li> <li>Shared Context \u2014 everyone sees releases, alerts, and recoveries in one thread.</li> <li>Safety Through Automation \u2014 Slack inputs are validated by backend handlers before triggering CI/CD pipelines.</li> <li>Cultural Alignment with DevOps \u2014 ChatOps reinforces DevOps values: collaboration, transparency, and continuous learning.</li> </ul>"},{"location":"chatops/chatops/#how-it-works-here","title":"How It Works Here","text":"<pre><code>graph LR\n    A[Developer] --&gt;|Submit form / slash command| B[Slack App]\n    B --&gt;|POST| C[Backend Handler]\n    C --&gt;|Dispatch| D[GitHub Actions / APIs]\n    D --&gt;|Status updates| B\n    B --&gt;|Message| A</code></pre>"},{"location":"chatops/chatops/#examples-in-this-demo","title":"Examples in This Demo","text":"<ul> <li>Release Modal \u2013 triggers a GitHub Actions workflow_dispatch to cut a new version tag and deploy it.</li> <li>Log Watch Alerts \u2013 15-minute scheduler posts error counts and log fingerprints to Slack channels.</li> <li>Acknowledgement Commands(TODO) \u2013 allow team members to mark alerts as handled directly in Slack threads.</li> </ul>"},{"location":"chatops/chatops/#future-extensions","title":"Future Extensions","text":"<ul> <li>Add approval gates for production releases via Slack buttons.</li> <li>Correlate alerts with Grafana dashboards using deep links.</li> </ul> <p>ChatOps here isn\u2019t another tool \u2014 it\u2019s the connective tissue between automation and human awareness. It turns conversations into action and keeps the operational pulse visible to everyone. This makes ChatOps stand on its own \u2014 philosophically close to DevOps, but documented as how collaboration and automation blend in my demo platform.</p>"},{"location":"deployment/kubernetes/","title":"Gitops","text":""},{"location":"deployment/kubernetes/#why-gitops-matters","title":"Why GitOps Matters","text":"<p>Traditional deployments rely on manual steps, imperative commands, and tribal knowledge. GitOps treats infrastructure and deployments as code\u2014every change is versioned, auditable, and reproducible.</p> <p>This platform demonstrates production GitOps patterns:</p> <ul> <li>Git as single source of truth - All infrastructure state is declared in this repository</li> <li>Automated reconciliation - ArgoCD continuously ensures cluster matches Git</li> <li>Self-service deployments - Push to main triggers the entire pipeline automatically</li> <li>Audit trail - Every deployment is a Git commit with full history</li> <li>Rollback capability - Revert a commit to roll back any change</li> </ul> <p>The result: deployments that are safer, faster, and require zero manual intervention.</p>"},{"location":"deployment/kubernetes/#the-stack","title":"The Stack","text":"Component Purpose What It Demonstrates GitHub Actions CI/CD pipelines Workflow automation, event-driven builds ARC Runners Self-hosted runners Cost optimization, in-cluster access DockerHub Container registry Image distribution, version management ArgoCD GitOps controller Declarative deployments, continuous sync Kustomize Manifest management Configuration management without templates Slack Notifications Observability into deployment pipeline"},{"location":"deployment/kubernetes/#deployment-architecture","title":"Deployment Architecture","text":"<pre><code>sequenceDiagram\n    participant Dev as Developer\n    participant GH as GitHub\n    participant Runner as ARC Runner\n    participant Hub as Docker Hub\n    participant Argo as ArgoCD\n    participant K8s as Kubernetes\n\n    Dev-&gt;&gt;GH: Push code\n    GH-&gt;&gt;Runner: Trigger workflow\n    Runner-&gt;&gt;Runner: Build image\n    Runner-&gt;&gt;Hub: Push image\n    Runner-&gt;&gt;GH: Update kustomization\n    GH-&gt;&gt;Argo: Detect change\n    Argo-&gt;&gt;K8s: Sync application\n    K8s-&gt;&gt;K8s: Deploy pods</code></pre>"},{"location":"deployment/kubernetes/#component-details","title":"Component Details","text":"GitHub Actions Workflows <p>Workflows are defined in <code>.github/workflows/</code> and trigger on pushes to main that modify specific paths.</p> <p>Example: <code>.github/workflows/todo-api.yaml</code> <pre><code>on:\n  push:\n    branches: [main]\n    paths:\n      - 'infra/homer/**'\n</code></pre></p> <p>Key workflows: - <code>homer.yaml</code>: Build and deploy homer - <code>argocd-update.yaml</code>: Reusable workflow for GitOps updates - <code>slack-notification.yaml</code>: Reusable workflow for Slack notifications</p> Self-Hosted ARC Runners <p>Actions Runner Controller (ARC) manages GitHub Actions runners inside the Kubernetes cluster.</p> <p>Configuration: - Location: <code>deploy/bootstrap/k8s/arc-configuration/gha-runner-scale-set/</code> - Namespace: <code>arc-runners</code> - Scale set name: <code>self-hosted</code> - Organization: <code>https://github.com/scottseotech</code> - Scaling: min 2, max 5 runners - Custom image: Built from <code>infra/runner/</code></p> <p>Why self-hosted runners? 1. In-cluster access to kubectl and kubeconfig 2. Faster builds with cached layers 3. Direct access to cluster services for verification 4. Cost control 5. Security (runs in isolated namespace with specific service account)</p> <p>Runner capabilities: - Docker-in-Docker (DinD) for building images - kubectl access for deployment verification - Git for repository operations - ArgoCD CLI for triggering syncs</p> DockerHub Registry <p>All container images are pushed to DockerHub under the <code>curiosinauts</code> organization.</p> <p>Image naming convention: <pre><code>curiosinauts/scottseotech-&lt;service-name&gt;:&lt;version&gt;\n</code></pre></p> <p>Examples: - <code>curiosinauts/scottseotech-todo-api:v0.7.10</code> - <code>curiosinauts/scottseotech-homer:def5678</code></p> <p>Version tags: - Git commit SHA (first 7 characters) - Ensures exact version traceability</p> <p>Registry credentials: - Stored as GitHub secrets: <code>DOCKERHUB_USERNAME</code>, <code>DOCKERHUB_TOKEN</code> - Injected into runner during workflow execution</p> ArgoCD GitOps <p>ArgoCD continuously monitors the Git repository and automatically syncs changes to the cluster.</p> <p>Configuration: - Namespace: <code>argocd</code> - Applications: Defined in <code>deploy/argocd-apps/</code> - Sync policy: Automatic with self-heal - Source: This repository (<code>main</code> branch) - Manifests: <code>deploy/argocd/applications/infrastructure/</code> and <code>deploy/argocd/manifests/infrastructure/</code></p> <p>App-of-Apps Pattern: ArgoCD uses an \"app-of-apps\" pattern where a root application (<code>argocd-apps</code>) manages all other applications.</p> <pre><code>argocd-apps (root)\n\u251c\u2500\u2500 todo-api\n\u251c\u2500\u2500 homer\n\u251c\u2500\u2500 cnpg\n\u251c\u2500\u2500 kube-prometheus-stack\n\u251c\u2500\u2500 loki\n\u2514\u2500\u2500 ...\n</code></pre> Kustomize <p>Kustomize manages Kubernetes manifests and image versions without templating.</p> <p>Structure per application: <pre><code>deploy/argocd/applications/infrastructure/todo-api/\n\u251c\u2500\u2500 kustomization.yaml    # Kustomize config with image reference\n\u251c\u2500\u2500 deployment.yaml        # Base deployment manifest\n\u251c\u2500\u2500 service.yaml           # Service definition\n\u2514\u2500\u2500 ...\n</code></pre></p> <p>Image management: Workflows update image versions using: <pre><code>kustomize edit set image todo-api=curiosinauts/scottseotech-todo-api:v0.7.10\n</code></pre></p> <p>This modifies <code>kustomization.yaml</code> without touching deployment manifests directly.</p> ARGOCD_LOCK Mechanism <p>The lock prevents concurrent deployments from interfering with each other.</p> <p>How it works: 1. Workflow attempts to acquire lock (GitHub repository variable) 2. Lock format: <code>&lt;app-name&gt;-&lt;run-id&gt;</code> 3. Retries every 10 seconds for up to 5 minutes 4. Proceeds with deployment if acquired 5. Always releases lock in cleanup step (even on failure)</p> <p>Why needed? - Prevents race conditions with multiple deployments - Ensures ArgoCD sync completes before next deployment - Avoids conflicts in Git repository updates</p> <p>Implementation: <pre><code>- name: Acquire Lock\n  run: |\n    for i in {1..30}; do\n      CURRENT_LOCK=$(gh variable get ARGOCD_LOCK)\n      if [ \"$CURRENT_LOCK\" = \"null\" ]; then\n        gh variable set ARGOCD_LOCK --body \"$APP_NAME-$RUN_ID\"\n        echo \"Lock acquired\"\n        break\n      fi\n      sleep 10\n    done\n</code></pre></p> Slack Notifications <p>All build and deployment results are posted to the <code>#builds</code> Slack channel.</p> <p>Notification types: - Build started - Build succeeded - Build failed - Deployment succeeded - Deployment failed</p> <p>Information included: - Service name - Version (commit SHA) - Status (success/failure) - Links to GitHub run</p> <p>Configuration: - Slack bot token: <code>SLACK_BOT_TOKEN</code> (GitHub secret) - Channel: <code>#builds</code> - Workflow: <code>.github/workflows/slack-notification.yaml</code></p>"},{"location":"deployment/kubernetes/#deployment-flow","title":"Deployment Flow","text":"<p>When you push code to main, here's what happens automatically:</p> <ol> <li>GitHub Actions detects the change and triggers the workflow</li> <li>Self-hosted runner builds the Docker image with commit SHA as version tag</li> <li>Image pushed to DockerHub registry</li> <li>Kustomize updates the image tag in Git</li> <li>ArgoCD detects the Git commit and syncs to cluster</li> <li>Kubernetes rolls out the new version with zero downtime</li> <li>Verification ensures the correct version is running</li> <li>Slack notification confirms success or reports failure</li> </ol> <p>Timeline: Typical deployment completes in 3-5 minutes from git push to running pods.</p> Detailed Step-by-Step Process <ol> <li> <p>Code Change <pre><code>git add infra/homer/\ngit commit -m \"feat: add new feature\"\ngit push origin main\n</code></pre></p> </li> <li> <p>Workflow Trigger</p> </li> <li>GitHub detects push to main</li> <li>Checks path filters</li> <li> <p>Queues job for self-hosted runner</p> </li> <li> <p>Runner Picks Up Job</p> </li> <li>ARC controller provisions or assigns existing runner pod</li> <li>Runner checks out code</li> <li> <p>Sets up environment</p> </li> <li> <p>Lock Acquisition <pre><code>LOCK_VALUE=\"homer-123456789\"\ngh variable set ARGOCD_LOCK --body \"$LOCK_VALUE\"\n</code></pre></p> </li> <li> <p>Docker Build <pre><code>VERSION=$(git rev-parse --short HEAD)\ndocker build -t curiosinauts/scottseotech-homer:$VERSION .\n</code></pre></p> </li> <li> <p>Docker Push <pre><code>docker push curiosinauts/scottseotech-homer:$VERSION\n</code></pre></p> </li> <li> <p>Kustomize Update <pre><code>cd deploy/argocd/manifests/infrastructure/homer\nkustomize edit set image homer=curiosinauts/scottseotech-homer:$VERSION\n</code></pre></p> </li> <li> <p>Git Commit <pre><code>git config user.name \"github-actions[bot]\"\ngit add deploy/argocd/manifests/infrastructure/homer/kustomization.yaml\ngit commit -m \"Update homer to version $VERSION\"\ngit push origin main\n</code></pre></p> </li> <li> <p>ArgoCD Sync</p> </li> <li>ArgoCD detects commit</li> <li>Automatically syncs application</li> <li> <p>Applies updated manifests to cluster</p> </li> <li> <p>Deployment Verification <pre><code>kubectl rollout status deployment/homer -n default --timeout=5m\nDEPLOYED_IMAGE=$(kubectl get deployment homer -n default -o jsonpath='{.spec.template.spec.containers[0].image}')\n</code></pre></p> </li> <li> <p>Lock Release <pre><code>gh variable set ARGOCD_LOCK --body \"null\"\n</code></pre></p> </li> <li> <p>Slack Notification</p> <ul> <li>Post success message with version</li> <li>Include links to GitHub run and commit</li> </ul> </li> </ol>"},{"location":"deployment/kubernetes/#configuration","title":"Configuration","text":"Required GitHub Secrets <pre><code>DOCKERHUB_USERNAME: DockerHub username\nDOCKERHUB_TOKEN: DockerHub access token\nARGOCD_TOKEN: ArgoCD API token\nGH_ACCESS_TOKEN: GitHub PAT for repo operations\nSLACK_BOT_TOKEN: Slack bot token for notifications\n</code></pre> Required GitHub Variables <pre><code>ARGOCD_LOCK: Deployment coordination lock (managed by workflows)\n</code></pre> Service Account Permissions <p>The ARC runner service account needs: <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: github-runner-deployment-reader\nrules:\n- apiGroups: [\"apps\"]\n  resources: [\"deployments\", \"statefulsets\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"get\", \"list\"]\n</code></pre></p>"},{"location":"deployment/kubernetes/#best-practices","title":"Best Practices","text":"<p>DO: - Always use <code>kustomize edit set image</code> to update versions - Let ArgoCD handle all deployments (don't <code>kubectl apply</code> manually) - Wait for deployment stability before releasing lock - Include meaningful commit messages for version updates - Monitor #builds channel for deployment status</p> <p>DO NOT: - Manually edit image tags in deployment manifests - Apply changes directly with kubectl (breaks GitOps) - Skip lock acquisition (causes race conditions) - Force push to main (breaks version history) - Use <code>--force</code> flags in git or kubectl commands</p>"},{"location":"deployment/kubernetes/#monitoring-deployments","title":"Monitoring Deployments","text":"Check Workflow Status <pre><code># List recent workflow runs\ngh run list --repo scottseotech/todo-platform\n\n# View specific run\ngh run view &lt;run-id&gt; --repo scottseotech/todo-platform\n\n# View logs\ngh run view &lt;run-id&gt; --log --repo scottseotech/todo-platform\n</code></pre> Check ArgoCD Status <pre><code># List applications\nkubectl get applications -n argocd\n\n# Check specific application\nkubectl get application todo-api -n argocd -o yaml\n\n# View sync status\nargocd app get todo-api\n</code></pre> Check Deployment Status <pre><code># Check deployment\nkubectl get deployment todo-api -n default\n\n# Check pods\nkubectl get pods -n default -l app=todo-api\n\n# View events\nkubectl get events -n default --sort-by='.lastTimestamp'\n\n# Check image version\nkubectl get deployment todo-api -n default -o jsonpath='{.spec.template.spec.containers[0].image}'\n</code></pre> Check Runner Status <pre><code># List runner pods\nkubectl get pods -n arc-runners\n\n# Check runner logs\nkubectl logs -n arc-runners &lt;runner-pod-name&gt; -c runner\n\n# View scale set status\nkubectl get runners -n arc-runners\n</code></pre>"},{"location":"deployment/kubernetes/#troubleshooting","title":"Troubleshooting","text":"Deployment Stuck at Lock Acquisition <p>Symptom: Workflow waiting indefinitely for ARGOCD_LOCK</p> <p>Solution: <pre><code># Check current lock\ngh variable get ARGOCD_LOCK --repo scottseotech/todo-platform\n\n# Manually release if stuck\ngh variable set ARGOCD_LOCK --body \"null\" --repo scottseotech/todo-platform\n</code></pre></p> ArgoCD Not Syncing <p>Symptom: Changes committed but ArgoCD not detecting them</p> <p>Solution: <pre><code># Check application status\nargocd app get todo-api\n\n# Manual sync\nargocd app sync todo-api\n\n# Hard refresh\nargocd app sync todo-api --force\n</code></pre></p> Deployment Failed Verification <p>Symptom: Deployment created but pods not running</p> <p>Solution: <pre><code># Check pod status\nkubectl get pods -n default -l app=todo-api\n\n# View pod logs\nkubectl logs -n default &lt;pod-name&gt;\n\n# Describe pod for events\nkubectl describe pod -n default &lt;pod-name&gt;\n\n# Check image pull\nkubectl get events -n default | grep -i pull\n</code></pre></p> Runner Not Picking Up Jobs <p>Symptom: Workflows queued but not starting</p> <p>Solution: <pre><code># Check runner pods\nkubectl get pods -n arc-runners\n\n# Check runner logs\nkubectl logs -n arc-runners &lt;runner-pod-name&gt;\n\n# Verify scale set\nkubectl get runnersets -n arc-runners\n\n# Check ARC controller\nkubectl logs -n arc-systems deployment/arc-controller-manager\n</code></pre></p> Image Push Failed <p>Symptom: Docker push fails during workflow</p> <p>Solution: 1. Verify DockerHub credentials in GitHub secrets 2. Check DockerHub rate limits 3. Verify network connectivity from runner 4. Check runner disk space:    <pre><code>kubectl exec -n arc-runners &lt;runner-pod&gt; -- df -h\n</code></pre></p> Slack Notification Not Sent <p>Symptom: Deployment completed but no Slack message</p> <p>Solution: 1. Verify SLACK_BOT_TOKEN is valid 2. Check bot is member of #builds channel 3. Review workflow logs for Slack API errors 4. Verify bot has <code>chat:write</code> scope</p>"},{"location":"deployment/kubernetes/#rolling-back-deployments","title":"Rolling Back Deployments","text":"Option 1: Revert Git Commit <pre><code># Find the commit to revert to\ngit log deploy/argocd/applications/infrastructure/todo-api/kustomization.yaml\n\n# Revert to previous version\ngit revert &lt;commit-sha&gt;\ngit push origin main\n\n# ArgoCD will automatically sync the rollback\n</code></pre> Option 2: Manual Kustomize Update <pre><code># Update to previous version\ncd deploy/argocd/applications/infrastructure/todo-api\nkustomize edit set image todo-api=curiosinauts/scottseotech-todo-api:&lt;previous-version&gt;\n\ngit add kustomization.yaml\ngit commit -m \"Rollback todo-api to &lt;previous-version&gt;\"\ngit push origin main\n</code></pre> Option 3: ArgoCD History Rollback <pre><code># View history\nargocd app history todo-api\n\n# Rollback to specific revision\nargocd app rollback todo-api &lt;revision-number&gt;\n</code></pre>"},{"location":"deployment/kubernetes/#advanced-topics","title":"Advanced Topics","text":"Adding New Services <ol> <li> <p>Create service directory structure:    <pre><code>mkdir -p deploy/argocd/applications/infrastructure/new-service\n</code></pre></p> </li> <li> <p>Add Kubernetes manifests (deployment, service, etc.)</p> </li> <li> <p>Create kustomization.yaml:    <pre><code>apiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nresources:\n  - deployment.yaml\n  - service.yaml\nimages:\n  - name: new-service\n    newName: curiosinauts/scottseotech-new-service\n    newTag: latest\n</code></pre></p> </li> <li> <p>Create ArgoCD application:    <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: new-service\n  namespace: argocd\nspec:\n  project: default\n  source:\n    repoURL: https://github.com/scottseotech/todo-platform\n    targetRevision: main\n    path: deploy/argocd/applications/infrastructure/new-service\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: default\n  syncPolicy:\n    automated:\n      prune: true\n      selfHeal: true\n</code></pre></p> </li> <li> <p>Create GitHub workflow following existing patterns</p> </li> </ol>"},{"location":"deployment/kubernetes/#security-considerations","title":"Security Considerations","text":"<ul> <li>Least Privilege: Runners have minimal RBAC permissions</li> <li>Secret Management: Secrets stored in GitHub, injected at runtime</li> <li>Network Policies: Runners isolated in dedicated namespace</li> <li>Signed Commits: Consider requiring GPG signatures</li> <li>Branch Protection: Protect main branch, require reviews</li> <li>Audit Logging: All deployments tracked in Git history</li> </ul>"},{"location":"deployment/kubernetes/#related-documentation","title":"Related Documentation","text":"<ul> <li>Getting Started - Development Setup</li> <li>Architecture Overview</li> <li>Observability &amp; Troubleshooting</li> </ul>"},{"location":"deployment/slack-driven-release/","title":"Slack-Driven Release Management","text":"<p>Releases shouldn\u2019t depend on whoever has the right terminal open. Putting a release modal in Slack gives developers a safe, auditable, self-service path to kick off the pipeline \u2014 right where the team already communicates. The result: less handoff friction, clearer visibility, and fewer \u201cwho pressed the button?\u201d moments.</p>"},{"location":"deployment/slack-driven-release/#what-it-does","title":"What It Does","text":"<ul> <li>A Slack interactive modal collects release inputs (service names, version).</li> <li>The modal\u2019s backend handler validates the request and invokes a GitHub Actions release workflow (e.g., build + publish + deploy).</li> <li>Status and links are echoed back to Slack for team visibility.</li> </ul>"},{"location":"deployment/slack-driven-release/#where-it-fits-in-devops","title":"Where It Fits in DevOps","text":"Concern Role of This Capability ChatOps Operational action (release) initiated and observed within Slack. Continuous Delivery / Release Automation Triggers a standardized GitHub Actions workflow to perform the actual release. Developer Self-Service Removes bottlenecks; no local tooling or special creds required by devs. Governance &amp; Audit Centralized entry point; who/when/what is captured in Slack and CI logs. Collaboration Real-time visibility of release start/completion in a shared channel."},{"location":"deployment/slack-driven-release/#high-level-flow","title":"High-Level Flow","text":"<pre><code>sequenceDiagram\n    participant Dev as Developer (Slack)\n    participant Bot as Slack App\n    participant GH as GitHub Actions\n    participant API as Release Handler\n\n    Dev-&gt;&gt;Bot: Open release modal\n    Bot--&gt;&gt;Dev: Prompt for service names/version\n    Dev-&gt;&gt;Bot: Submit form\n    Bot-&gt;&gt;API: POST form payload\n    API-&gt;&gt;GH: Dispatch workflow (workflow_dispatch)\n    GH--&gt;&gt;API: 202 Accepted (run_id)\n    API--&gt;&gt;Bot: Ack + post message with run link\n    API-&gt;&gt;GH: Poll for workflow run completion\n    API--&gt;&gt;Bot: Post on 'deployments' channel when run completes</code></pre>"},{"location":"deployment/slack-driven-release/#example-release-modal-in-slack","title":"Example: Release Modal in Slack","text":"View Screenshots <p> Slack interactive modal for triggering releases </p> <p> Dispatch workflow </p> <p> Post message with run id </p> <p> Post on deployment channel when run completes </p>"},{"location":"development/getting-started/","title":"Getting Started with Development","text":""},{"location":"development/getting-started/#why-develop-locally","title":"Why Develop Locally?","text":"<p>Working with this platform locally gives you hands-on experience with production-grade development practices:</p> <p>Learn by doing - Instead of just reading about GitOps, observability, and cloud-native patterns, you'll actually work with them. Make changes, see traces flow through Grafana, trigger GitOps deployments, and debug distributed systems.</p> <p>Iterate quickly - Hot reloading with Air for Go services means your changes appear instantly. No waiting for container rebuilds during development. Test MCP integrations, modify API endpoints, and experiment with observability without the CI/CD overhead.</p> <p>Understand the full stack - Run the entire system locally\u2014from AI assistant to database\u2014and see how components interact. Debug trace propagation, test backup procedures, and understand how Kubernetes operators work in a safe environment.</p> <p>This isn't just running code. It's practicing the skills that matter in production systems.</p>"},{"location":"development/getting-started/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following installed:</p> <ul> <li>K3s - for running the platform in a local Kubernetes cluster</li> <li>kubectl - for Kubernetes cluster interaction</li> <li>Go 1.21 or higher - for Go services</li> <li>Python 3.11 or higher - for Python CLI tools</li> <li>Docker - for building and testing containers</li> <li>PostgreSQL - for local database development (optional)</li> <li>Git - for version control</li> </ul>"},{"location":"development/getting-started/#what-youll-work-with","title":"What You'll Work With","text":"<p>This platform is organized around production patterns, not just application code:</p> <p>Application code - Go REST API with OpenTelemetry tracing, MCP server for AI integration, Python operational tools. You'll see how to structure services for observability and maintainability.</p> <p>Infrastructure as code - ArgoCD applications, Kustomize overlays, CloudNativePG database configs. You'll learn how infrastructure is declared, versioned, and deployed through GitOps.</p> <p>Automated operations - Bootstrap scripts, GitHub Actions workflows with self-hosted runners, backup automation. You'll understand how to automate the operational complexity of production systems.</p> <p>The repository structure reflects how production systems are actually built and operated, not just how code is written.</p>"},{"location":"development/getting-started/#local-kubernetes-setup","title":"Local Kubernetes Setup","text":"<p>Experience the full platform stack with a lightweight K3s cluster running on your machine.</p> Installing K3s <p>K3s is a lightweight Kubernetes distribution perfect for local development and testing.</p> <p>Install K3s: <pre><code>curl -sfL https://get.k3s.io | sh -\n</code></pre></p> <p>Verify installation: <pre><code>sudo k3s kubectl get nodes\n</code></pre></p> <p>Set up kubectl access: <pre><code># Copy k3s kubeconfig to default location\nmkdir -p ~/.kube\nsudo cp /etc/rancher/k3s/k3s.yaml ~/.kube/config\nsudo chown $USER:$USER ~/.kube/config\n\n# Test kubectl\nkubectl get nodes\n</code></pre></p> <p>Why K3s?</p> <ul> <li>Lightweight: Single binary, minimal resource usage (~512MB RAM)</li> <li>Fast: Cluster starts in seconds, not minutes</li> <li>Production-like: Real Kubernetes, not a simulation</li> <li>Self-contained: Everything runs locally, no cloud dependencies</li> </ul> Deploying the Platform to K3s <p>Once K3s is running, deploy the full stack using the bootstrap script:</p> <pre><code># Navigate to bootstrap directory\ncd deploy/bootstrap\n\n# Run the bootstrap script\n./bootstrap.sh\n</code></pre> <p>This installs:</p> <ul> <li>ArgoCD - GitOps controller for automated deployments</li> <li>GitHub Actions Runners (ARC) - Self-hosted CI/CD runners</li> <li>CloudNativePG - PostgreSQL operator for database HA</li> <li>Observability Stack - Prometheus, Tempo, Loki, Grafana</li> <li>MinIO - S3-compatible storage for backups and logs</li> </ul> <p>The bootstrap process demonstrates infrastructure-as-code and operator patterns in action.</p> Working with Your Local Cluster <p>Check cluster status: <pre><code>kubectl get pods -A\nkubectl get applications -n argocd\n</code></pre></p> <p>Access services: <pre><code># Grafana (monitoring)\nkubectl port-forward -n monitoring svc/kube-prometheus-stack-grafana 3000:80\n\n# ArgoCD (deployments)\nkubectl port-forward -n argocd svc/argocd-server 8080:80\n</code></pre></p> <p>Stop K3s: <pre><code>sudo systemctl stop k3s\n</code></pre></p> <p>Uninstall K3s: <pre><code>/usr/local/bin/k3s-uninstall.sh\n</code></pre></p>"},{"location":"development/getting-started/#golang-development-setup","title":"Golang Development Setup","text":"<p>Learn production Go development with dependency management, hot reloading, and environment configuration.</p> Initial Setup <ol> <li> <p>Navigate to the Go service directory: <pre><code>cd services/todo-api\n</code></pre></p> </li> <li> <p>Download and verify dependencies: <pre><code>go mod download\n</code></pre></p> </li> <li> <p>Clean up and verify <code>go.mod</code> and <code>go.sum</code>: <pre><code>go mod tidy\n</code></pre></p> </li> </ol> <p>This ensures all dependencies are properly resolved and recorded.</p> Running the Service Locally <p>Create a <code>.env</code> file: <pre><code>cp .env.example .env\n# Edit .env with your local configuration\n</code></pre></p> <p>Option 1: Standard Go Run <pre><code>go run main.go\n</code></pre></p> <p>The server will start on <code>http://localhost:8080</code>.</p> <p>Option 2: Hot Reloading with Air</p> <p>Air provides live reloading during development, automatically rebuilding and restarting when you save changes.</p> <ol> <li> <p>Install Air: <pre><code>go install github.com/air-verse/air@latest\n</code></pre></p> </li> <li> <p>Ensure <code>$GOPATH/bin</code> or <code>$HOME/go/bin</code> is in your PATH: <pre><code>export PATH=$PATH:$(go env GOPATH)/bin\n</code></pre></p> </li> <li> <p>Initial air configuration: <pre><code>cd services/todo-api\nair init\n</code></pre></p> </li> <li> <p>Run with Air: <pre><code>air\n</code></pre></p> </li> </ol> <p>Air will watch for file changes and automatically rebuild and restart the service.</p> <p>Configuration: Air looks for <code>.air.toml</code> in the service directory. The default configuration typically works well, but you can customize:</p> <ul> <li>Build commands</li> <li>File watch patterns</li> <li>Excluded directories</li> <li>Delay between rebuilds</li> </ul>"},{"location":"development/getting-started/#python-development-setup","title":"Python Development Setup","text":"<p>Build operational CLI tools with Python virtual environments, editable installs, and environment configuration.</p> Setting Up todops-cli <p>The <code>todops</code> CLI tool is used for platform operations like searching logs, managing ignore lists, and posting to Slack.</p> <ol> <li> <p>Navigate to the CLI directory: <pre><code>cd apps/todops-cli\n</code></pre></p> </li> <li> <p>Create a virtual environment: <pre><code>python3 -m venv venv\n</code></pre></p> </li> <li> <p>Activate the virtual environment: <pre><code>source venv/bin/activate\n</code></pre></p> </li> <li> <p>Install in editable mode (for development): <pre><code>pip install -e .\n</code></pre></p> </li> </ol> <p>The <code>-e</code> flag installs the package in \"editable\" mode, meaning changes to the source code are immediately reflected without needing to reinstall.</p> Development Workflow <ol> <li>Make changes to Python code in the <code>apps/todops-cli/</code> directory</li> <li>Changes are immediately available (no reinstall needed with <code>-e</code> flag)</li> <li>Test with: <code>todops --help</code> or any todops command</li> </ol> <p>Example workflow: <pre><code># Activate venv\nsource venv/bin/activate\n\n# Make changes to code\nvi todops/loki_commands.py\n\n# Test immediately\ntodops loki search \"error\" --since \"1h\"\n\n# Deactivate when done\ndeactivate\n</code></pre></p> Environment Variables <p>Create a <code>.env</code> file in <code>apps/todops-cli/</code> for local development:</p> <pre><code># Loki configuration\nLOKI_URL=http://localhost:3100\n\n# MinIO configuration (for ignore list storage)\nMINIO_URL=localhost:9000\nMINIO_ACCESS_KEY=your-access-key\nMINIO_SECRET_KEY=your-secret-key\n\n# Slack configuration (for notifications)\nSLACK_BOT_TOKEN=xoxb-your-slack-bot-token\n</code></pre> <p>The CLI will automatically load these from <code>.env</code> when running locally.</p>"},{"location":"development/getting-started/#docker-development","title":"Docker Development","text":"<p>Practice containerization and image building for local testing before CI/CD pipelines.</p> Building Images Locally <p>Todo API: <pre><code>cd services/todo-api\ndocker build -t todo-api:local .\n</code></pre></p> <p>todops-cli (built as part of Dagu): <pre><code>cd infra/dagu\ndocker build -t dagu:local .\n</code></pre></p>"},{"location":"development/getting-started/#database-setup","title":"Database Setup","text":"<p>Understand how to run PostgreSQL locally and connect to production CloudNativePG clusters.</p> Local PostgreSQL <p>If running services locally, you'll need a PostgreSQL database.</p> <p>Using Docker: <pre><code>docker run --name todo-postgres \\\n  -e POSTGRES_USER=postgres \\\n  -e POSTGRES_PASSWORD=postgres \\\n  -e POSTGRES_DB=todo_db \\\n  -p 5432:5432 \\\n  -d postgres:15\n</code></pre></p> <p>Connect from your service: <pre><code>export SERVER_PORT=8080\nexport DB_HOST=localhost\nexport DB_PORT=5432\nexport DB_USER=postgres\nexport DB_PASSWORD=postgres\nexport DB_NAME=todo_db\nexport DB_SSLMODE=disable\nexport OTEL_ENABLE_STDOUT=true\n</code></pre></p> Cluster Database <p>In the Kubernetes cluster, services connect to CloudNativePG:</p> <ul> <li>Connection: <code>todo-db-rw.cnpg.svc.cluster.local:5432</code></li> <li>Database: <code>todo_db</code></li> <li>Credentials: Stored in <code>todo-api-secret</code> Kubernetes secret</li> </ul>"},{"location":"development/getting-started/#port-forwarding-for-local-development","title":"Port Forwarding for Local Development","text":"<p>Access observability tools, databases, and storage from your local machine while they run in the cluster.</p> Port Forwarding Commands <p>To access cluster services from your local machine:</p> <pre><code># Loki (Logs)\nkubectl port-forward -n logging svc/loki-gateway 3100:80\n\n# MinIO (Object Storage)\nkubectl port-forward -n minio svc/minio 9000:9000 9001:9001\n\n# PostgreSQL (Database)\nkubectl port-forward -n cnpg svc/todo-db-rw 5432:5432\n\n# Grafana (Dashboards)\nkubectl port-forward -n monitoring svc/kube-prometheus-stack-grafana 3000:80\n</code></pre>"},{"location":"development/getting-started/#what-to-explore-next","title":"What to Explore Next","text":"<p>Now that you have the platform running locally, here's how to get the most value:</p> <p>Understand the System Design \u2192 See how components connect, understand data flows, and learn why architectural decisions were made. This shows how to think about system design, not just implementation.</p> <p>Deploy Like Production \u2192 Experience the full GitOps workflow: make a code change, push to Git, watch ArgoCD sync, verify deployment, see observability data flow. This is how real deployments work.</p> <p>Debug with Observability \u2192 Make an API call and watch the trace flow from MCP \u2192 API \u2192 Database \u2192 Grafana. Practice using distributed tracing to debug issues before they hit production.</p> <p>Practice makes permanent - The more you experiment with this stack locally, the more comfortable you'll be building and operating production systems.</p>"},{"location":"observability/log-monitoring-and-alerting/","title":"Log Monitoring and Alerting","text":""},{"location":"observability/log-monitoring-and-alerting/#why-it-matters","title":"Why It Matters","text":"<p>Modern observability stacks are powerful, but dashboards don\u2019t always catch issues fast enough. This scheduled job exists to close that awareness gap \u2014 automatically surfacing log anomalies in real time where the team already works: Slack.</p> <p>The intent is simple: errors should never wait for someone to open Grafana. By scanning logs on a regular cadence, the system provides a steady heartbeat of operational health, reinforcing reliability and proactive detection.</p> <p>Context-Rich Alerts \u2014 messages summarize error patterns to reduce alert noise. Low Overhead \u2014 lightweight automation without needing a full log analysis pipeline.</p>"},{"location":"observability/log-monitoring-and-alerting/#what-it-does","title":"What It Does","text":"<p>Every 15 minutes, a Dagu <code>Job</code> scans the most recent logs for the keyword <code>error</code>. It aggregates and reports:</p> <ul> <li>Total count of matching log entries</li> <li>Signature of each unique error pattern</li> </ul> <p>The job formats these results into a Slack message and posts them to an alert channel through a bot token.</p>"},{"location":"observability/log-monitoring-and-alerting/#how-it-fits-the-devops-stack","title":"How It Fits the DevOps Stack","text":"Concern Role of This Job Observability Continuously inspects logs for new or recurring error patterns. Incident Response Bridges detection and communication, reducing mean time to awareness (MTTA). Automation Fully automated via Dagu Job, no manual checks needed. ChatOps Publishes real-time error summaries directly into Slack. Reliability Engineering Adds redundancy to monitoring\u2014serves as a fallback when dashboards are quiet."},{"location":"observability/log-monitoring-and-alerting/#example-slack-alert","title":"Example Slack Alert","text":"<pre><code>9 [default/todo-bot] --- stderr F from todoclientmcp import TodoMCPClient, MCPError\n\n4 [default/todo-bot] --- stderr F openai.OpenAIError: The api_key client option must be set...\n</code></pre>"},{"location":"observability/observability/","title":"Observability","text":""},{"location":"observability/observability/#why-observability-matters","title":"Why Observability Matters","text":"<p>Production systems fail in unexpected ways. Observability - the ability to understand internal system state from external outputs - is what separates systems that are debuggable from those that aren't.</p> <p>This platform implements the three pillars of observability with full integration:</p> <ul> <li>Traces: Follow requests across services (MCP \u2192 API \u2192 Database)</li> <li>Logs: Centralized log aggregation with full-text search</li> <li>Metrics: Time-series data for performance and health monitoring</li> </ul> <p>All telemetry flows to Grafana for unified visualization and correlation.</p>"},{"location":"observability/observability/#the-stack","title":"The Stack","text":"Component Purpose What It Demonstrates OpenTelemetry Instrumentation SDK Vendor-neutral observability, W3C trace propagation Tempo Trace storage Distributed tracing backend, correlation with logs Loki Log aggregation Cost-effective log storage, label-based querying Prometheus Metrics collection Time-series monitoring, alerting rules Grafana Visualization Unified dashboards, trace/log/metric correlation <p>Tip</p> <p>Click the node description to jump to the detailed explanation section.</p>"},{"location":"observability/observability/#observability-architecture","title":"Observability Architecture","text":"<pre><code>graph TB\n    subgraph \"Application Layer\"\n        MCP[MCP Server&lt;br/&gt;Port 8081]\n        API[Todo API&lt;br/&gt;Port 8080]\n        DB[(PostgreSQL&lt;br/&gt;CNPG)]\n    end\n\n    subgraph \"Instrumentation\"\n        OTEL[OpenTelemetry SDK&lt;br/&gt;OTLP Exporter]\n    end\n\n    subgraph \"Telemetry Collection\"\n        FB[Fluent Bit&lt;br/&gt;Log Collector]\n        PROM[Prometheus&lt;br/&gt;Metrics Scraper]\n        TEMPO[Tempo&lt;br/&gt;Trace Collector]\n    end\n\n    subgraph \"Storage &amp; Processing\"\n        LOKI[Loki&lt;br/&gt;Log Storage]\n        PROMDB[(Prometheus&lt;br/&gt;TSDB)]\n        TEMPODB[(Tempo&lt;br/&gt;Trace DB)]\n    end\n\n    subgraph \"Visualization\"\n        GRAFANA[Grafana&lt;br/&gt;Unified Dashboard]\n    end\n\n    MCP --&gt;|traces| OTEL\n    API --&gt;|traces| OTEL\n    API --&gt; DB\n\n    MCP --&gt;|logs| FB\n    API --&gt;|logs| FB\n    DB --&gt;|logs| FB\n\n    MCP --&gt;|/metrics| PROM\n    API --&gt;|/metrics| PROM\n    DB --&gt;|/metrics| PROM\n\n    OTEL --&gt;|OTLP/gRPC&lt;br/&gt;:4317| TEMPO\n    FB --&gt;|HTTP&lt;br/&gt;:3100| LOKI\n    PROM --&gt;|scrape| PROMDB\n    TEMPO --&gt; TEMPODB\n\n    TEMPODB --&gt;|query| GRAFANA\n    LOKI --&gt;|query| GRAFANA\n    PROMDB --&gt;|query| GRAFANA\n\n    click OTEL \"/observability/observability/#opentelemetry-implementation\" \"tool tip\"\n    click TEMPODB \"/observability/observability/#tempo-distributed-tracing\" \"tool tip\"\n    click LOKI \"/observability/observability/#loki-log-aggregation\" \"tool tip\"\n    click PROMDB \"/observability/observability/#prometheus-metrics\" \"tool tip\"\n    click GRAFANA \"/observability/observability/#grafana-unified-dashboard\" \"tool tip\"\n</code></pre> <p>Data Flow:</p> <ul> <li>Traces: Applications emit OTLP traces \u2192 Tempo \u2192 Grafana</li> <li>Logs: Applications write JSON logs \u2192 Fluent Bit \u2192 Loki \u2192 Grafana</li> <li>Metrics: Applications expose <code>/metrics</code> \u2192 Prometheus scrapes \u2192 Grafana</li> </ul> <p>All three signals are correlated in Grafana by trace ID for unified debugging.</p>"},{"location":"observability/observability/#opentelemetry-implementation","title":"OpenTelemetry Implementation","text":"<p>OpenTelemetry provides vendor-neutral instrumentation for traces, logs, and metrics.</p> Trace Propagation <p>W3C Trace Context propagates trace IDs across service boundaries:</p> <pre><code>Trace Flow 1: Slack Bot \u2192 MCP Server (HTTP/SSE)\n              TraceID: abc123\n\nTrace Flow 2: MCP Server \u2192 API \u2192 Database (HTTP/REST)\n              TraceID: def456 (new trace, MCP protocol boundary)\n\nTrace-to-Log Correlation:\n- All logs include trace_id and span_id\n- Search Loki by trace_id to see all logs for a request\n</code></pre> <p>Why two trace flows? The MCP protocol runs JSON-RPC over Server-Sent Events (SSE), which is a long-lived streaming connection. Trace context is not propagated through the JSON-RPC message payloads, creating a natural boundary. However, each flow is fully traced and correlated with logs.</p> <p>Each service extracts the trace context from incoming HTTP requests and injects it into outgoing HTTP requests.</p> <p>Implementation:</p> <pre><code>// In todo-api and todo-mcp\nimport \"go.opentelemetry.io/otel\"\n\n// Extract context from incoming HTTP request\nctx := otel.GetTextMapPropagator().Extract(r.Context(), propagation.HeaderCarrier(r.Header))\n\n// Start a new span with extracted context\nctx, span := tracer.Start(ctx, \"operation-name\")\ndefer span.End()\n</code></pre> Exporter Configuration <p>Traces are exported via OTLP/gRPC to Tempo:</p> <pre><code>exporter, err := otlptracegrpc.New(ctx,\n    otlptracegrpc.WithEndpoint(\"tempo:4317\"),\n    otlptracegrpc.WithInsecure(),\n)\n\ntp := sdktrace.NewTracerProvider(\n    sdktrace.WithBatcher(exporter),\n    sdktrace.WithResource(resource.NewWithAttributes(\n        semconv.ServiceNameKey.String(\"todo-api\"),\n    )),\n)\n</code></pre> <p>Configuration: - Endpoint: Tempo collector on port 4317 - Protocol: OTLP/gRPC - Batching: 5-second interval or 512 spans - Sampling: 100% (all traces recorded)</p> Adding Custom Spans <p>Create spans for important operations:</p> <pre><code>func CreateTodo(ctx context.Context, title string) error {\n    ctx, span := tracer.Start(ctx, \"CreateTodo\")\n    defer span.End()\n\n    span.SetAttributes(\n        attribute.String(\"todo.title\", title),\n        attribute.String(\"user.id\", userID),\n    )\n\n    // Business logic here\n    if err != nil {\n        span.RecordError(err)\n        span.SetStatus(codes.Error, \"failed to create todo\")\n        return err\n    }\n\n    span.AddEvent(\"todo_created\", trace.WithAttributes(\n        attribute.Int(\"todo.id\", todoID),\n    ))\n\n    return nil\n}\n</code></pre> Performance Tuning <p>Sampling: For high-traffic production, reduce sampling: <pre><code>sdktrace.WithSampler(sdktrace.ParentBased(\n    sdktrace.TraceIDRatioBased(0.1), // Sample 10% of traces\n))\n</code></pre></p> <p>Batch Processing: <pre><code>sdktrace.WithBatcher(exporter,\n    sdktrace.WithBatchTimeout(5*time.Second),\n    sdktrace.WithMaxExportBatchSize(512),\n)\n</code></pre></p> <p>High Overhead? - Reduce sampling rate - Increase batch timeout - Disable stdout exporter in production</p>"},{"location":"observability/observability/#tempo-distributed-tracing","title":"Tempo - Distributed Tracing","text":"<p>Tempo stores and queries distributed traces from OpenTelemetry.</p> Configuration <ul> <li>Storage: Local filesystem (configurable for S3/GCS)</li> <li>Ingestion: OTLP/gRPC on port 4317</li> <li>Query: Grafana Tempo data source</li> <li>Retention: Configurable based on storage</li> </ul> Querying Traces <p>In Grafana: 1. Navigate to Explore 2. Select Tempo data source 3. Search by:     - Trace ID     - Service name     - Duration     - Tags/attributes</p> <p>Example queries: - Find slow requests: <code>{duration &gt; 1s}</code> - Find errors: <code>{status = error}</code> - By service: <code>{service.name = \"todo-api\"}</code></p>"},{"location":"observability/observability/#loki-log-aggregation","title":"Loki - Log Aggregation","text":"<p>Centralized log collection with S3 storage backend.</p> Log Collection <p>Fluent Bit agents ship logs to Loki:</p> <ul> <li>Collects structured logs from all services</li> <li>Parses JSON log format</li> <li>Adds Kubernetes metadata (pod, namespace, labels)</li> <li>Ships to Loki gateway</li> </ul> Storage <ul> <li>Backend: MinIO S3-compatible storage</li> <li>Retention: Configurable based on S3 lifecycle policies</li> <li>Compression: Optimized for cost-effective long-term storage</li> </ul> Querying Logs <p>LogQL query examples: <pre><code># All logs from todo-api\n{app=\"todo-api\"}\n\n# Errors only\n{app=\"todo-api\"} |= \"error\"\n\n# Specific trace ID\n{app=\"todo-api\"} | json | trace_id=\"abc123\"\n\n# Rate of errors\nsum(rate({app=\"todo-api\"} |= \"error\" [5m]))\n</code></pre></p>"},{"location":"observability/observability/#prometheus-metrics","title":"Prometheus - Metrics","text":"<p>Time-series metrics collection and alerting.</p> Scraping Configuration <p>Prometheus scrapes metrics every 15 seconds:</p> <ul> <li>Todo API: Go runtime metrics, HTTP request metrics</li> <li>Todo MCP: Go runtime metrics, MCP operation metrics</li> <li>PostgreSQL: Database performance, replication lag, connection pool stats</li> </ul> Available Metrics <p>Go Runtime: - <code>go_goroutines</code> - Number of goroutines - <code>go_memstats_alloc_bytes</code> - Memory allocated - <code>go_gc_duration_seconds</code> - GC pause duration</p> <p>HTTP: - <code>http_requests_total</code> - Request counter - <code>http_request_duration_seconds</code> - Request latency - <code>http_requests_in_flight</code> - Concurrent requests</p> <p>PostgreSQL: - <code>cnpg_pg_replication_lag</code> - Replication lag - <code>cnpg_pg_database_size_bytes</code> - Database size - <code>cnpg_pg_stat_database_blks_read</code> - Disk reads</p>"},{"location":"observability/observability/#grafana-unified-dashboard","title":"Grafana - Unified Dashboard","text":"<p>Single pane of glass for traces, logs, and metrics.</p> Data Sources <ul> <li>Prometheus: Metrics and alerts</li> <li>Tempo: Distributed traces</li> <li>Loki: Log aggregation</li> </ul> Trace-to-Log Correlation <p>Click on a trace span in Grafana to see associated logs:</p> <ol> <li>View trace in Tempo</li> <li>Click on a span</li> <li>See \"Logs for this span\" link</li> <li>Automatically queries Loki with matching trace ID</li> </ol> <p>This demonstrates how to correlate telemetry for debugging.</p> Alerting <p>Alerts route to Slack <code>#alerts</code> channel:</p> <ul> <li>error log patterns are used to aggregate same type of logs to include counts in alerts</li> </ul>"},{"location":"observability/observability/#troubleshooting-observability","title":"Troubleshooting Observability","text":"<p>Verify telemetry is flowing correctly.</p> Test Trace Propagation Locally <p>1. Start services with stdout tracing: <pre><code>export OTEL_ENABLE_STDOUT=true\ncd services/todo-api &amp;&amp; go run main.go\ncd services/todo-mcp &amp;&amp; go run main.go\n</code></pre></p> <p>2. Trigger a request: <pre><code>curl -X POST http://localhost:8081/tools/todos-add/invoke \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"arguments\": {\"title\": \"Test trace\", \"due_date\": \"2025-11-01T10:00:00Z\"}}'\n</code></pre></p> <p>3. Verify trace IDs match: Check stdout output from both services - trace ID should be identical.</p> Test Against Kubernetes <p>1. Port forward to MCP service: <pre><code>kubectl port-forward -n default svc/todo-mcp 8081:8081\n</code></pre></p> <p>2. Trigger test request: <pre><code>curl -X POST http://localhost:8081/tools/todos-add/invoke \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"arguments\": {\"title\": \"Test K8s trace\", \"due_date\": \"2025-11-01T10:00:00Z\"}}'\n</code></pre></p> <p>3. View trace in Grafana: - Port forward Grafana: <code>kubectl port-forward -n monitoring svc/kube-prometheus-stack-grafana 3000:80</code> - Navigate to Explore \u2192 Tempo - Search for recent traces from <code>todo-mcp</code></p> Common Issues <p>No traces appearing: - Check Tempo endpoint: <code>kubectl logs -n monitoring deployment/tempo</code> - Verify OTLP endpoint: Should be <code>tempo:4317</code> - Check exporter config in service logs</p> <p>Trace IDs don't match: - Verify W3C trace context propagation - Check HTTP header injection/extraction - Ensure context is passed through function calls</p> <p>High overhead: - Enable sampling: Reduce to 10% or lower - Increase batch timeout - Disable stdout exporter</p>"},{"location":"observability/observability/#best-practices","title":"Best Practices","text":"<ol> <li>Always propagate context - Pass <code>context.Context</code> through all function calls</li> <li>Use descriptive span names - <code>CreateTodo</code> not <code>Handler</code></li> <li>Add meaningful attributes - Include IDs, user info, business context</li> <li>Record errors - Always call <code>span.RecordError(err)</code> on errors</li> <li>Set span status - Use <code>codes.Ok</code> or <code>codes.Error</code></li> <li>Keep spans focused - One logical operation per span</li> <li>Avoid sensitive data - Don't include passwords, tokens in attributes</li> <li>Use correlation - Link traces to logs with trace IDs</li> </ol>"},{"location":"observability/observability/#references","title":"References","text":"<ul> <li>OpenTelemetry Go Documentation</li> <li>OTLP Specification</li> <li>Grafana Tempo</li> <li>W3C Trace Context</li> <li>Prometheus Best Practices</li> <li>LogQL Syntax</li> </ul>"}]}